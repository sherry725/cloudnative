GPT-3: 模型能力不一致
ChatGPT: 用RLHF强化学习对其进一步训练，解决能力不一致的问题
强化学习:智能体agent在与环境的交互过程中通过学习策略以达成回报最大化（目标是追求最大回报）或实现特定目标
policy 函数：控制agent, 当状态s1带入policy函数计算概率，抽样得到行为a1，环境生成下一状态s2并给agent一个奖励r1，将s2带入policy函数，抽样得到a2,s3,e2, etc
ChatGPT强化学习步骤
1. 监督学习，SFT，在少量已标注的数据上进行调优
   通过人为标注问题和答案，实现大模型的微调
   基线模型  text-davinci-003
2. 训练奖励模型，对大量的SFT模型输出进行收集和排序，在这个新数据集上(大概是SFT数据集的10倍)训练新模型，也就是训练奖励模型
   人为输入问题 -> SFT模型 -> 一系列答案，给这些答案人为排序 -> 奖励模型（GPT-3的蒸馏版本）-> 预测值（得分）
   loss(theta) = -(1/(K2))E(x,yw,yl)[log(sigma(r(x,yw)-r(x,yl)))]
   x:问题，yw:SFT模型给出的一个好结果， yl:SFT模型给出的另外一个结果（差），r:代表模型
   r(x,yw)：模型给好的结果打分， r(x,yw)-r(x,yl) 越大越好， loss越小越好
   奖励模型训练就是给SFT模型的输出进行打分，SFT回答的较好，奖励模型分数就打得高
   https://zhuanlan.zhihu.com/p/644409015
3. 强化学习，PPO算法，对奖励模型进一步调优和改进SFT模型，PPO输出的结果是策略模式
   输入问题 -> PPO模型（可以理解为ChatGPT模型） -> 结果输入到奖励模型 -> 奖励分数，如果分数比较低，需要利用PPO算法更新ChatGPT模型参数

ChatGLM-6B
GLM: 基于自回归空白填充的通用预训练框架，mask一个或多个词，然后训练模型预测mask的词
